{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns \n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #% matplotlib inline\n",
    "from art import tprint\n",
    "from sklearn import svm \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import pylab\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split # для манипулирования данными\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns \n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #% matplotlib inline\n",
    "from art import tprint\n",
    "\n",
    "# url =\"C:\\\\Users\\\\Admin\\\\Desktop\\\\datases\\\\UMAP\\\\soybean-large.data\"  # hone pc\n",
    "# url_2= \"C:\\\\Users\\\\Admin\\\\Desktop\\\\datases\\\\UMAP\\\\soybean-large.names\"  # hone pc\n",
    "\n",
    "\n",
    "url =\"C:\\\\Users\\\\Дмитрий\\\\Desktop\\\\datasets\\\\bob\\\\soybean-large.data\"  # laptop\n",
    "url_2= \"C:\\\\Users\\\\Дмитрий\\\\Desktop\\\\datasets\\\\bob\\\\soybean-large.names\"  # laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_func(df):\n",
    "    scaler = MinMaxScaler()\n",
    "# X= scaler.fit_transform(y_pred_SVM)\n",
    "\n",
    "    embed = TSNE(\n",
    "        n_components=2, # значение по умолчанию=2. Размерность вложенного пространства.\n",
    "        perplexity=20, # значение по умолчанию=30.0. Перплексия связана с количеством ближайших соседей, которое используется в других алгоритмах обучения на множествах.\n",
    "        early_exaggeration=12, # значение по умолчанию=12.0. Определяет, насколько плотными будут естественные кластеры исходного пространстве во вложенном пространстве и сколько места будет между ними. \n",
    "        learning_rate=200, # значение по умолчанию=200.0. Скорость обучения для t-SNE обычно находится в диапазоне [10.0, 1000.0]. Если скорость обучения слишком высока, данные могут выглядеть как \"шар\", в котором любая точка приблизительно равноудалена от ближайших соседей. Если скорость обучения слишком низкая, большинство точек могут быть похожими на сжатое плотное облако с незначительным количеством разбросов. \n",
    "        n_iter=5000, # значение по умолчанию=1000. Максимальное количество итераций для оптимизации. Должно быть не менее 250.\n",
    "        n_iter_without_progress=300, # значение по умолчанию=300. Максимальное количество итераций без прогресса перед прекращением оптимизации, используется после 250 начальных итераций с ранним преувеличением.\n",
    "        min_grad_norm=0.0000001, # значение по умолчанию=1e-7. Если норма градиента ниже этого порога, оптимизация будет остановлена.\n",
    "        metric='euclidean', # значение по умолчанию='euclidean', Метрика, используемая при расчете расстояния между экземплярами в массиве признаков.\n",
    "        init='random',# {'random', 'pca'} или ndarray формы (n_samples, n_components), значение по умолчанию='random'. Инициализация вложения.\n",
    "        verbose=0, # значение по умолчанию=0. Уровень детализации.\n",
    "        random_state=42, # экземпляр RandomState или None, по умолчанию=None. Определяет генератор случайных чисел. Передача int для воспроизводимых результатов при многократном вызове функции.\n",
    "        method='barnes_hut', # значение по умолчанию='barnes_hut'. По умолчанию алгоритм вычисления градиента использует аппроксимацию Барнса-Хата, работающую в течение времени O(NlogN). метод='exact' будет работать по более медленному, но точному алгоритму за время O(N^2). Следует использовать точный алгоритм, когда количество ошибок ближайших соседей должно быть ниже 3%.\n",
    "        angle=0.5, # значение по умолчанию=0.5. Используется только если метод='barnes_hut' Это компромисс между скоростью и точностью в случае T-SNE с применением алгоритма Барнса-Хата.\n",
    "        n_jobs=-1, # значение по умолчанию=None. Количество параллельных заданий для поиска соседей. -1 означает использование всех процессоров.\n",
    "            )\n",
    "\n",
    "    # Преобразование X\n",
    "    X_embedded = embed.fit_transform(df.reshape(-1, 1))\n",
    "    return X_embedded\n",
    "\n",
    "def umap_function(df, method='MinMax'):\n",
    "\n",
    "    # if method=='MinMax':\n",
    "    #     scaler = MinMaxScaler()\n",
    "    # elif method=='Standard':\n",
    "    #     scaler = preprocessing.StandardScaler()\n",
    "    # else:\n",
    "    #     scaler = preprocessing.RobustScaler()\n",
    "    \n",
    "    # X= scaler.fit_transform(df.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "    manifold = umap.UMAP()#.fit(X)\n",
    "    X_reduced = manifold.fit_transform(df.reshape(-1, 1))\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [   \"class\",\"date\", \"plant-stand\", \"precip\", \"temp\", \"hail\", \"crop-hist\", \"area-damaged\", \"severity\", \"seed-tmt\", \"germination\", \"plant-growth\", \n",
    "                \"leaves\",\"leafspots-halo\", \"leafspots-marg\", \"leafspot-size\", \"leaf-shread\", \"leaf-malf\", \"leaf-mild\", \"stem\", \"lodging\", \"stem-cankers\",\n",
    "                \"canker-lesion\", \"fruiting-bodies\", \"external decay\", \"mycelium\", \"int-discolor\", \"sclerotia\", \"fruit-pods\", \"fruit spots\", \"seed\",\n",
    "                \"mold-growth\", \"seed-discolor\", \"seed-size\", \"shriveling\", \"roots\"\n",
    "            ]\n",
    "\n",
    "# Read data from URL\n",
    "bob = pd.read_csv(url, names=col_names)\n",
    "color=list(bob)\n",
    "bob=bob.replace(\"?\",0)\n",
    "# bob=bob[color[1:]] \n",
    "# \n",
    "list_class=bob['class'].to_list()\n",
    "dict_class={}\n",
    "a=1\n",
    "for i in list_class:\n",
    " \n",
    "    if i not in dict_class.keys():\n",
    "        dict_class[i]=a\n",
    "        a+=1\n",
    "bob['class']=bob['class'].apply(lambda x: dict_class[x] )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
