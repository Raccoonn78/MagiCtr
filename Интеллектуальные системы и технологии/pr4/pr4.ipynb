{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Дмитрий\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns \n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #% matplotlib inline\n",
    "from art import tprint\n",
    "from sklearn import svm \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import pylab\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split # для манипулирования данными\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns \n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #% matplotlib inline\n",
    "from art import tprint\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# url =\"C:\\\\Users\\\\Admin\\\\Desktop\\\\datases\\\\UMAP\\\\soybean-large.data\"  # hone pc\n",
    "# url_2= \"C:\\\\Users\\\\Admin\\\\Desktop\\\\datases\\\\UMAP\\\\soybean-large.names\"  # hone pc\n",
    "\n",
    "\n",
    "url =\"C:\\\\Users\\\\Дмитрий\\\\Desktop\\\\datasets\\\\bob\\\\soybean-large.data\"  # laptop\n",
    "url_2= \"C:\\\\Users\\\\Дмитрий\\\\Desktop\\\\datasets\\\\bob\\\\soybean-large.names\"  # laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_func(df):\n",
    "    scaler = MinMaxScaler()\n",
    "# X= scaler.fit_transform(y_pred_SVM)\n",
    "\n",
    "    embed = TSNE(\n",
    "        n_components=2, # значение по умолчанию=2. Размерность вложенного пространства.\n",
    "        perplexity=20, # значение по умолчанию=30.0. Перплексия связана с количеством ближайших соседей, которое используется в других алгоритмах обучения на множествах.\n",
    "        early_exaggeration=12, # значение по умолчанию=12.0. Определяет, насколько плотными будут естественные кластеры исходного пространстве во вложенном пространстве и сколько места будет между ними. \n",
    "        learning_rate=200, # значение по умолчанию=200.0. Скорость обучения для t-SNE обычно находится в диапазоне [10.0, 1000.0]. Если скорость обучения слишком высока, данные могут выглядеть как \"шар\", в котором любая точка приблизительно равноудалена от ближайших соседей. Если скорость обучения слишком низкая, большинство точек могут быть похожими на сжатое плотное облако с незначительным количеством разбросов. \n",
    "        n_iter=5000, # значение по умолчанию=1000. Максимальное количество итераций для оптимизации. Должно быть не менее 250.\n",
    "        n_iter_without_progress=300, # значение по умолчанию=300. Максимальное количество итераций без прогресса перед прекращением оптимизации, используется после 250 начальных итераций с ранним преувеличением.\n",
    "        min_grad_norm=0.0000001, # значение по умолчанию=1e-7. Если норма градиента ниже этого порога, оптимизация будет остановлена.\n",
    "        metric='euclidean', # значение по умолчанию='euclidean', Метрика, используемая при расчете расстояния между экземплярами в массиве признаков.\n",
    "        init='random',# {'random', 'pca'} или ndarray формы (n_samples, n_components), значение по умолчанию='random'. Инициализация вложения.\n",
    "        verbose=0, # значение по умолчанию=0. Уровень детализации.\n",
    "        random_state=42, # экземпляр RandomState или None, по умолчанию=None. Определяет генератор случайных чисел. Передача int для воспроизводимых результатов при многократном вызове функции.\n",
    "        method='barnes_hut', # значение по умолчанию='barnes_hut'. По умолчанию алгоритм вычисления градиента использует аппроксимацию Барнса-Хата, работающую в течение времени O(NlogN). метод='exact' будет работать по более медленному, но точному алгоритму за время O(N^2). Следует использовать точный алгоритм, когда количество ошибок ближайших соседей должно быть ниже 3%.\n",
    "        angle=0.5, # значение по умолчанию=0.5. Используется только если метод='barnes_hut' Это компромисс между скоростью и точностью в случае T-SNE с применением алгоритма Барнса-Хата.\n",
    "        n_jobs=-1, # значение по умолчанию=None. Количество параллельных заданий для поиска соседей. -1 означает использование всех процессоров.\n",
    "            )\n",
    "\n",
    "    # Преобразование X\n",
    "    X_embedded = embed.fit_transform(df.reshape(-1, 1))\n",
    "    return X_embedded\n",
    "\n",
    "def umap_function(df, method='MinMax'):\n",
    "\n",
    "    # if method=='MinMax':\n",
    "    #     scaler = MinMaxScaler()\n",
    "    # elif method=='Standard':\n",
    "    #     scaler = preprocessing.StandardScaler()\n",
    "    # else:\n",
    "    #     scaler = preprocessing.RobustScaler()\n",
    "    \n",
    "    # X= scaler.fit_transform(df.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "    manifold = umap.UMAP()#.fit(X)\n",
    "    X_reduced = manifold.fit_transform(df.reshape(-1, 1))\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaves\n",
      "Counter({'4': 9, '1': 5, '5': 5, '3': 4, '6': 3, '0': 2, '2': 2})\n"
     ]
    }
   ],
   "source": [
    "col_names = [   \"class\",\"date\", \"plant-stand\", \"precip\", \"temp\", \"hail\", \"crop-hist\", \"area-damaged\", \"severity\", \"seed-tmt\", \"germination\", \"plant-growth\", \n",
    "                \"leaves\",\"leafspots-halo\", \"leafspots-marg\", \"leafspot-size\", \"leaf-shread\", \"leaf-malf\", \"leaf-mild\", \"stem\", \"lodging\", \"stem-cankers\",\n",
    "                \"canker-lesion\", \"fruiting-bodies\", \"external decay\", \"mycelium\", \"int-discolor\", \"sclerotia\", \"fruit-pods\", \"fruit spots\", \"seed\",\n",
    "                \"mold-growth\", \"seed-discolor\", \"seed-size\", \"shriveling\", \"roots\"\n",
    "            ]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Read data from URL\n",
    "bob = pd.read_csv(url, names=col_names)\n",
    "color=list(bob)\n",
    "bob=bob.replace(\"?\",0)\n",
    "bob=bob.dropna()\n",
    "\n",
    "for i in list(bob):\n",
    "    try:\n",
    "        bob[i]=bob[i].str.strip() \n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "list_class=bob['class'].to_list()\n",
    "dict_class={}\n",
    "a=1\n",
    "for i in list_class:\n",
    " \n",
    "    if i not in dict_class.keys():\n",
    "        dict_class[i]=a\n",
    "        a+=1\n",
    "# bob=bob.fillna(0)\n",
    "bob=bob.dropna()\n",
    "bob['class']=bob['class'].apply(lambda x: dict_class[x] )  \n",
    "bob=bob.dropna()\n",
    "bob= bob[ (bob['class']== 6) |(bob['class']== 11) |(bob['class']== 7) | (bob['class']==10 ) |(bob['class']== 9) |(bob['class']== 3) ] # (bob['class']==2 ) |\n",
    "\n",
    "predictors, target =bob[color[1:]], bob['class'] #  делим на то что есть и на то что предсказать \n",
    "\n",
    "x_train , x_test , y_train, y_test = train_test_split(predictors['date'], target, random_state=100, test_size = 0.5) # 1А разбить выборку на тренировочную и тестовую \n",
    "X, y = train_test_split(bob, random_state=100)\n",
    "x_test=x_test.values.reshape(-1, 1)\n",
    "\n",
    "print(Counter(x_train)) # узнать сколько классов \n",
    "\n",
    "# y_train=y_train[y_train[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictors, target =bob[color[1:]], bob['class'] #  делим на то что есть и на то что предсказать \n",
    "x_train , x_test , y_train, y_test = train_test_split(predictors, target, random_state=0) # 1А разбить выборку на тренировочную и тестовую \n",
    "X, y = train_test_split(bob, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE()\n",
    "x_train, y_train = sm.fit_resample(x_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "# x_train=x_train.to_frame()\n",
    "sm = SMOTE( k_neighbors = 2, random_state = 100) \n",
    "X_diver_res, y_diver_res = sm.fit_resample(x_train, y_train.ravel())  # из массива массивов делает одномерный массив (ravel)\n",
    "sm = BorderlineSMOTE( k_neighbors = 2,random_state=100, kind='borderline-1')\n",
    "X_res, y_res = sm.fit_resample(x_train, y_train.ravel())\n",
    "\n",
    "sm = BorderlineSMOTE( k_neighbors = 2,random_state=100, kind='borderline-1')\n",
    "X_res_2, y_res_2 = sm.fit_resample(x_train, y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     10\n",
      "1     10\n",
      "2      3\n",
      "3      6\n",
      "4      9\n",
      "5     11\n",
      "6      7\n",
      "7     10\n",
      "8     11\n",
      "9      6\n",
      "10    11\n",
      "11     7\n",
      "12     6\n",
      "13     3\n",
      "14     9\n",
      "15     6\n",
      "16    11\n",
      "17     7\n",
      "18    11\n",
      "19     3\n",
      "20     6\n",
      "21     7\n",
      "22     9\n",
      "23     6\n",
      "24    10\n",
      "25    10\n",
      "26    11\n",
      "27     9\n",
      "28     3\n",
      "29     6\n",
      "30    10\n",
      "31     7\n",
      "32     3\n",
      "33     7\n",
      "34     9\n",
      "35     7\n",
      "36     6\n",
      "37     3\n",
      "38     9\n",
      "39    11\n",
      "40     3\n",
      "41     3\n",
      "42    11\n",
      "43    10\n",
      "44    10\n",
      "45     7\n",
      "46     9\n",
      "47     9\n",
      "Name: class, dtype: int64\n",
      "\n",
      "           SVM       KNN  Random_Forest\n",
      "precision  1.0  0.946667            1.0\n",
      "recall     1.0  0.933333            1.0\n",
      "fscore     1.0  0.925926            1.0\n",
      "accuracy   1.0  0.933333            1.0\n",
      "[10 10  3  6  9 11  7 10 11  6 11  7  6  3  9  6 11  7 11  3  6  7  9  6\n",
      " 10 10 11  9  3  6 10  7  3  7  9  7  6  3  9 11  3  3 11 10 10  7  9  9]\n",
      " ____   __  __   ___   _____  _____ \n",
      "/ ___| |  \\/  | / _ \\ |_   _|| ____|\n",
      "\\___ \\ | |\\/| || | | |  | |  |  _|  \n",
      " ___) || |  | || |_| |  | |  | |___ \n",
      "|____/ |_|  |_| \\___/   |_|  |_____|\n",
      "                                    \n",
      "\n",
      "           SVM       KNN  Random_Forest\n",
      "precision  1.0  0.946667            1.0\n",
      "recall     1.0  0.933333            1.0\n",
      "fscore     1.0  0.925926            1.0\n",
      "accuracy   1.0  0.933333            1.0\n",
      "[10 10  3  6  9 11  7 10 11  6 11  7  6  3  9  6 11  7 11  3  6  7  9  6\n",
      " 10 10 11  9  3  6 10  7  3  7  9  7  6  3  9 11  3  3 11 10 10  7  9  9]\n",
      " _                       _              _  _                      ____   __  __   ___   _____  _____  \n",
      "| |__    ___   _ __   __| |  ___  _ __ | |(_) _ __    ___        / ___| |  \\/  | / _ \\ |_   _|| ____| \n",
      "| '_ \\  / _ \\ | '__| / _` | / _ \\| '__|| || || '_ \\  / _ \\ _____ \\___ \\ | |\\/| || | | |  | |  |  _|   \n",
      "| |_) || (_) || |   | (_| ||  __/| |   | || || | | ||  __/|_____| ___) || |  | || |_| |  | |  | |___  \n",
      "|_.__/  \\___/ |_|    \\__,_| \\___||_|   |_||_||_| |_| \\___|       |____/ |_|  |_| \\___/   |_|  |_____| \n",
      "                                                                                                      \n",
      "\n",
      "           SVM       KNN  Random_Forest\n",
      "precision  1.0  0.946667            1.0\n",
      "recall     1.0  0.933333            1.0\n",
      "fscore     1.0  0.925926            1.0\n",
      "accuracy   1.0  0.933333            1.0\n",
      "[10 10  3  6  9 11  7 10 11  6 11  7  6  3  9  6 11  7 11  3  6  7  9  6\n",
      " 10 10 11  9  3  6 10  7  3  7  9  7  6  3  9 11  3  3 11 10 10  7  9  9]\n",
      " _                       _              _  _                      ____   __  __   ___   _____  _____          ____  \n",
      "| |__    ___   _ __   __| |  ___  _ __ | |(_) _ __    ___        / ___| |  \\/  | / _ \\ |_   _|| ____|        |___ \\ \n",
      "| '_ \\  / _ \\ | '__| / _` | / _ \\| '__|| || || '_ \\  / _ \\ _____ \\___ \\ | |\\/| || | | |  | |  |  _|            __) |\n",
      "| |_) || (_) || |   | (_| ||  __/| |   | || || | | ||  __/|_____| ___) || |  | || |_| |  | |  | |___          / __/ \n",
      "|_.__/  \\___/ |_|    \\__,_| \\___||_|   |_||_||_| |_| \\___|       |____/ |_|  |_| \\___/   |_|  |_____|  _____ |_____|\n",
      "                                                                                                      |_____|       \n",
      "\n",
      "           SVM       KNN  Random_Forest\n",
      "precision  1.0  0.946667            1.0\n",
      "recall     1.0  0.933333            1.0\n",
      "fscore     1.0  0.925926            1.0\n",
      "accuracy   1.0  0.933333            1.0\n"
     ]
    }
   ],
   "source": [
    "for x_new_train,y_new_train, version in zip([x_train, X_diver_res, X_res, X_res_2],[y_train, y_diver_res, y_res, y_res_2], ['ДЕФОЛТ','SMOTE','borderline-SMOTE ','borderline-SMOTE _2']): \n",
    "    try:\n",
    "        # x_new_train=x_new_train.values.reshape(-1, 1)\n",
    "        pass\n",
    "    except:\n",
    "        ...\n",
    "\n",
    "    params={'kernel':['linear','rbf' ,  'poly', 'sigmoid'], 'C': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]}\n",
    "\n",
    "    clf_1_SVC = svm.SVC(kernel='linear')\n",
    "    clf_2_SVC = svm.SVC(kernel='rbf') \n",
    "    clf_3_SVC = svm.SVC(kernel='poly')\n",
    "    print(y_new_train)\n",
    "    clf_predict=clf_1_SVC.fit(x_new_train,y_new_train)\n",
    "    clf_predict_2=clf_2_SVC.fit(x_new_train,y_new_train)\n",
    "    clf_predict_3=clf_3_SVC.fit(x_new_train,y_new_train)\n",
    "    clf_SVC = svm.SVC()\n",
    "    y_pred_SVM= clf_predict.predict(x_test)\n",
    "    y_pred_SVM_2= clf_predict_2.predict(x_test)\n",
    "    y_pred_SVM_3= clf_predict_3.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, algorithm='auto')\n",
    "    knn_2 = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "    knn_3 = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
    "    knn_model = knn.fit(x_new_train,y_new_train)\n",
    "    knn_model_2 = knn_2.fit(x_new_train,y_new_train)\n",
    "    knn_model_3 = knn_3.fit(x_new_train,y_new_train)\n",
    "    y_pred_KNN = knn.predict(x_test)\n",
    "    y_pred_KNN_2 = knn_2.predict(x_test)\n",
    "    y_pred_KNN_3 = knn_3.predict(x_test)\n",
    "    params={'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "    grid_search= GridSearchCV(estimator=knn, param_grid=params)\n",
    "    grid_search.fit(x_new_train,y_new_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    random_forest = RandomForestClassifier(n_estimators=100 , criterion='gini')\n",
    "    random_forest_2 = RandomForestClassifier(n_estimators=100, criterion='entropy' )\n",
    "    random_forest_3 = RandomForestClassifier(n_estimators=100 , criterion='log_loss')\n",
    "    random_forest.fit(x_new_train,y_new_train)\n",
    "    random_forest_2.fit(x_new_train,y_new_train)\n",
    "    random_forest_3.fit(x_new_train,y_new_train)\n",
    "    y_pred_RF = random_forest.predict(x_test)\n",
    "    y_pred_RF_2 = random_forest_2.predict(x_test)\n",
    "    y_pred_RF_3 = random_forest_3.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "    params= {'criterion':['gini', 'entropy', 'log_loss'] }\n",
    "    grid_search= GridSearchCV(estimator=random_forest, param_grid=params)\n",
    "    grid_search.fit(x_new_train,y_new_train)\n",
    "\n",
    "\n",
    "\n",
    "    metrics = []\n",
    "    models =  ['SVM' , 'KNN', 'Random_Forest']\n",
    "    predictions=[y_pred_SVM, y_pred_KNN, y_pred_RF]\n",
    "    tprint(version)\n",
    "    for lab,i in zip(models, predictions):\n",
    "        precision, recall, fscore, _ = score(y_test, i, average='weighted')\n",
    "        accuracy = accuracy_score(y_test, i)\n",
    "\n",
    "        \n",
    "        metrics.append(pd.Series({  'precision':precision,  # Точность \n",
    "                                    'recall':recall, # полнота  \n",
    "                                    'fscore':fscore,\n",
    "                                    'accuracy':accuracy,}, name=lab))\n",
    "\n",
    "    metrics = pd.concat(metrics, axis=1)\n",
    "\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-мера является хорошим кандидатом на формальную метрику оценки качества классификатора. Она сводит к одному числу две других основополагающих метрики: точность и полноту. Имея в своем распоряжении подобный механизм оценки вам будет гораздо проще принять решение о том являются ли изменения в алгоритме в лучшую сторону или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10,  3,  6,  9, 11,  7, 10, 11,  6, 11,  7,  6,  3,  9,  6, 11,\n",
       "        7, 11,  3,  6,  7,  9,  6, 10, 10, 11,  9,  3,  6, 10,  7,  3,  7,\n",
       "        9,  7,  6,  3,  9, 11,  3,  3, 11, 10, 10,  7,  9,  9],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# plt.scatter(X_diver_res.reshape(1, -1)[0,:] ,y_diver_res , c=[i for i in y_diver_res], s=50, cmap='autumn')\n",
    "# plt.scatter(X_diver_res ,y_diver_res , c=[i for i in y_diver_res], s=50, cmap='autumn')\n",
    "# X_diver_res\n",
    "y_diver_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.        , 4.        , 1.        , 6.        , 1.        ,\n",
       "       4.        , 0.        , 4.        , 5.        , 1.        ,\n",
       "       5.        , 2.        , 2.        , 3.        , 4.        ,\n",
       "       5.        , 4.        , 4.        , 6.        , 3.        ,\n",
       "       3.        , 5.        , 5.        , 4.        , 6.        ,\n",
       "       4.        , 4.        , 0.        , 1.        , 1.        ,\n",
       "       1.        , 4.        , 4.        , 4.85139516, 4.        ,\n",
       "       4.        ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_diver_res.reshape(1, -1)[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 10,  6,  6,  3,  9,  3,  7, 10,  7,  7,  3,  6,  9, 11, 11, 11,\n",
       "        6, 11,  3,  9,  9,  6, 10,  6,  7,  9,  3,  7,  3,  7, 10, 10, 10,\n",
       "       11, 11], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_diver_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'kernel':['linear','rbf' ,  'poly', 'sigmoid'], 'C': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]}\n",
    "\n",
    "\n",
    "clf_1_SVC = svm.SVC(kernel='linear')\n",
    "clf_2_SVC = svm.SVC(kernel='rbf') \n",
    "clf_3_SVC = svm.SVC(kernel='poly')\n",
    "clf_predict=clf_1_SVC.fit(x_train,y_train)\n",
    "clf_predict_2=clf_2_SVC.fit(x_train,y_train)\n",
    "clf_predict_3=clf_3_SVC.fit(x_train,y_train)\n",
    "\n",
    "clf_SVC = svm.SVC()\n",
    "y_pred_SVM= clf_predict.predict(x_test)\n",
    "y_pred_SVM_2= clf_predict_2.predict(x_test)\n",
    "y_pred_SVM_3= clf_predict_3.predict(x_test)\n",
    "\n",
    "# grid_search= GridSearchCV(estimator=clf_SVC, param_grid=params)\n",
    "# grid_search.fit(x_train,y_train)\n",
    "# grid_search\n",
    "# print(str(grid_search.best_score_))\n",
    "# print(str(grid_search.best_estimator_))\n",
    "\n",
    "# y_pred_SVM= grid_search.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, algorithm='auto')\n",
    "knn_2 = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "knn_3 = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
    "\n",
    "knn_model = knn.fit(x_train, y_train)\n",
    "knn_model_2 = knn_2.fit(x_train, y_train)\n",
    "knn_model_3 = knn_3.fit(x_train, y_train)\n",
    "\n",
    "y_pred_KNN = knn.predict(x_test)\n",
    "y_pred_KNN_2 = knn_2.predict(x_test)\n",
    "y_pred_KNN_3 = knn_3.predict(x_test)\n",
    "\n",
    "\n",
    "params={'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "grid_search= GridSearchCV(estimator=knn, param_grid=params)\n",
    "grid_search.fit(x_train,y_train)\n",
    "print(str(grid_search.best_score_))\n",
    "print(str(grid_search.best_estimator_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100 , criterion='gini')\n",
    "random_forest_2 = RandomForestClassifier(n_estimators=100, criterion='entropy' )\n",
    "random_forest_3 = RandomForestClassifier(n_estimators=100 , criterion='log_loss')\n",
    "random_forest.fit(x_train, y_train)\n",
    "random_forest_2.fit(x_train, y_train)\n",
    "random_forest_3.fit(x_train, y_train)\n",
    "y_pred_RF = random_forest.predict(x_test)\n",
    "y_pred_RF_2 = random_forest_2.predict(x_test)\n",
    "y_pred_RF_3 = random_forest_3.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "params= {'criterion':['gini', 'entropy', 'log_loss'] }\n",
    "grid_search= GridSearchCV(estimator=random_forest, param_grid=params)\n",
    "grid_search.fit(x_train,y_train)\n",
    "print(str(grid_search.best_score_))\n",
    "print(str(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##################Последнее задание ######################\\n')\n",
    "metrics = []\n",
    "models =  ['SVM' , 'KNN', 'Random_Forest']\n",
    "predictions=[y_pred_SVM, y_pred_KNN, y_pred_RF]\n",
    "\n",
    "for lab,i in zip(models, predictions):\n",
    "    precision, recall, fscore, _ = score(y_test, i, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, i)\n",
    "\n",
    "    \n",
    "    metrics.append(pd.Series({  'precision':precision, \n",
    "                                'recall':recall,\n",
    "                                'fscore':fscore,\n",
    "                                'accuracy':accuracy,}, name=lab))\n",
    "\n",
    "metrics = pd.concat(metrics, axis=1)\n",
    "\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_df1=pd.DataFrame(tsne_func(y_pred_SVM), columns=['x1','x2']) # y_pred_SVM_2\n",
    "x_df2=pd.DataFrame(tsne_func(y_pred_SVM_2), columns=['x1','x2']) \n",
    "x_df3=pd.DataFrame(tsne_func(y_pred_SVM_3), columns=['x1','x2']) \n",
    "tprint(\"TSNE   SVM\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('SVM linear')\n",
    "ax2.set_title('SVM rbf')\n",
    "ax3.set_title('SVM poly')\n",
    "\n",
    "sns.kdeplot(x_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(x_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(x_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(x_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(x_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(x_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_df1=pd.DataFrame(tsne_func(y_pred_KNN), columns=['x1','x2']) # y_pred_SVM_2\n",
    "y_df2=pd.DataFrame(tsne_func(y_pred_KNN_2), columns=['x1','x2']) \n",
    "y_df3=pd.DataFrame(tsne_func(y_pred_KNN_3), columns=['x1','x2']) \n",
    "tprint(\"TSNE   KNN\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('KNN auto') #'auto', 'ball_tree', 'kd_tree',\n",
    "ax2.set_title('KNN rbf')\n",
    "ax3.set_title('KNN ball_tree')\n",
    "\n",
    "sns.kdeplot(y_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(y_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(y_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(y_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(y_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(y_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "z_df1=pd.DataFrame(tsne_func(y_pred_RF), columns=['x1','x2']) # y_pred_SVM_2\n",
    "z_df2=pd.DataFrame(tsne_func(y_pred_RF_3), columns=['x1','x2']) \n",
    "z_df3=pd.DataFrame(tsne_func(y_pred_RF_3), columns=['x1','x2']) \n",
    "tprint(\"TSNE   RandomForest\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('RandomForest gini') # 'gini', 'entropy', 'log_loss'\n",
    "ax2.set_title('RandomForest entropy')\n",
    "ax3.set_title('RandomForest log_loss')\n",
    "\n",
    "sns.kdeplot(z_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(z_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(z_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(z_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(z_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(z_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_df1=pd.DataFrame(umap_function(y_pred_SVM), columns=['x1','x2']) # y_pred_SVM_2\n",
    "x_df2=pd.DataFrame(umap_function(y_pred_SVM_2), columns=['x1','x2']) \n",
    "x_df3=pd.DataFrame(umap_function(y_pred_SVM_3), columns=['x1','x2']) \n",
    "tprint(\"UMAP   SVM\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('SVM linear')\n",
    "ax2.set_title('SVM rbf')\n",
    "ax3.set_title('SVM poly')\n",
    "\n",
    "sns.kdeplot(x_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(x_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(x_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(x_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(x_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(x_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_df1=pd.DataFrame(umap_function(y_pred_KNN), columns=['x1','x2']) # y_pred_SVM_2\n",
    "y_df2=pd.DataFrame(umap_function(y_pred_KNN_2), columns=['x1','x2']) \n",
    "y_df3=pd.DataFrame(umap_function(y_pred_KNN_3), columns=['x1','x2']) \n",
    "tprint(\"UMAP   KNN\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('KNN auto') #'auto', 'ball_tree', 'kd_tree',\n",
    "ax2.set_title('KNN rbf')\n",
    "ax3.set_title('KNN ball_tree')\n",
    "\n",
    "sns.kdeplot(y_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(y_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(y_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(y_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(y_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(y_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "z_df1=pd.DataFrame(umap_function(y_pred_RF), columns=['x1','x2']) # y_pred_SVM_2\n",
    "z_df2=pd.DataFrame(umap_function(y_pred_RF_3), columns=['x1','x2']) \n",
    "z_df3=pd.DataFrame(umap_function(y_pred_RF_3), columns=['x1','x2']) \n",
    "tprint(\"UMAP   RandomForest\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('RandomForest gini') # 'gini', 'entropy', 'log_loss'\n",
    "ax2.set_title('RandomForest entropy')\n",
    "ax3.set_title('RandomForest log_loss')\n",
    "\n",
    "sns.kdeplot(z_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(z_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(z_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(z_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(z_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(z_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
