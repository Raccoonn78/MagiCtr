{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Дмитрий\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns \n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #% matplotlib inline\n",
    "from art import tprint\n",
    "from sklearn import svm \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import pylab\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split # для манипулирования данными\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns \n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #% matplotlib inline\n",
    "from art import tprint\n",
    "\n",
    "# url =\"C:\\\\Users\\\\Admin\\\\Desktop\\\\datases\\\\UMAP\\\\soybean-large.data\"  # hone pc\n",
    "# url_2= \"C:\\\\Users\\\\Admin\\\\Desktop\\\\datases\\\\UMAP\\\\soybean-large.names\"  # hone pc\n",
    "\n",
    "\n",
    "url =\"C:\\\\Users\\\\Дмитрий\\\\Desktop\\\\datasets\\\\bob\\\\soybean-large.data\"  # laptop\n",
    "url_2= \"C:\\\\Users\\\\Дмитрий\\\\Desktop\\\\datasets\\\\bob\\\\soybean-large.names\"  # laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_func(df):\n",
    "    scaler = MinMaxScaler()\n",
    "# X= scaler.fit_transform(y_pred_SVM)\n",
    "\n",
    "    embed = TSNE(\n",
    "        n_components=2, # значение по умолчанию=2. Размерность вложенного пространства.\n",
    "        perplexity=20, # значение по умолчанию=30.0. Перплексия связана с количеством ближайших соседей, которое используется в других алгоритмах обучения на множествах.\n",
    "        early_exaggeration=12, # значение по умолчанию=12.0. Определяет, насколько плотными будут естественные кластеры исходного пространстве во вложенном пространстве и сколько места будет между ними. \n",
    "        learning_rate=200, # значение по умолчанию=200.0. Скорость обучения для t-SNE обычно находится в диапазоне [10.0, 1000.0]. Если скорость обучения слишком высока, данные могут выглядеть как \"шар\", в котором любая точка приблизительно равноудалена от ближайших соседей. Если скорость обучения слишком низкая, большинство точек могут быть похожими на сжатое плотное облако с незначительным количеством разбросов. \n",
    "        n_iter=5000, # значение по умолчанию=1000. Максимальное количество итераций для оптимизации. Должно быть не менее 250.\n",
    "        n_iter_without_progress=300, # значение по умолчанию=300. Максимальное количество итераций без прогресса перед прекращением оптимизации, используется после 250 начальных итераций с ранним преувеличением.\n",
    "        min_grad_norm=0.0000001, # значение по умолчанию=1e-7. Если норма градиента ниже этого порога, оптимизация будет остановлена.\n",
    "        metric='euclidean', # значение по умолчанию='euclidean', Метрика, используемая при расчете расстояния между экземплярами в массиве признаков.\n",
    "        init='random',# {'random', 'pca'} или ndarray формы (n_samples, n_components), значение по умолчанию='random'. Инициализация вложения.\n",
    "        verbose=0, # значение по умолчанию=0. Уровень детализации.\n",
    "        random_state=42, # экземпляр RandomState или None, по умолчанию=None. Определяет генератор случайных чисел. Передача int для воспроизводимых результатов при многократном вызове функции.\n",
    "        method='barnes_hut', # значение по умолчанию='barnes_hut'. По умолчанию алгоритм вычисления градиента использует аппроксимацию Барнса-Хата, работающую в течение времени O(NlogN). метод='exact' будет работать по более медленному, но точному алгоритму за время O(N^2). Следует использовать точный алгоритм, когда количество ошибок ближайших соседей должно быть ниже 3%.\n",
    "        angle=0.5, # значение по умолчанию=0.5. Используется только если метод='barnes_hut' Это компромисс между скоростью и точностью в случае T-SNE с применением алгоритма Барнса-Хата.\n",
    "        n_jobs=-1, # значение по умолчанию=None. Количество параллельных заданий для поиска соседей. -1 означает использование всех процессоров.\n",
    "            )\n",
    "\n",
    "    # Преобразование X\n",
    "    X_embedded = embed.fit_transform(df.reshape(-1, 1))\n",
    "    return X_embedded\n",
    "\n",
    "def umap_function(df, method='MinMax'):\n",
    "\n",
    "    # if method=='MinMax':\n",
    "    #     scaler = MinMaxScaler()\n",
    "    # elif method=='Standard':\n",
    "    #     scaler = preprocessing.StandardScaler()\n",
    "    # else:\n",
    "    #     scaler = preprocessing.RobustScaler()\n",
    "    \n",
    "    # X= scaler.fit_transform(df.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "    manifold = umap.UMAP()#.fit(X)\n",
    "    X_reduced = manifold.fit_transform(df.reshape(-1, 1))\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [   \"class\",\"date\", \"plant-stand\", \"precip\", \"temp\", \"hail\", \"crop-hist\", \"area-damaged\", \"severity\", \"seed-tmt\", \"germination\", \"plant-growth\", \n",
    "                \"leaves\",\"leafspots-halo\", \"leafspots-marg\", \"leafspot-size\", \"leaf-shread\", \"leaf-malf\", \"leaf-mild\", \"stem\", \"lodging\", \"stem-cankers\",\n",
    "                \"canker-lesion\", \"fruiting-bodies\", \"external decay\", \"mycelium\", \"int-discolor\", \"sclerotia\", \"fruit-pods\", \"fruit spots\", \"seed\",\n",
    "                \"mold-growth\", \"seed-discolor\", \"seed-size\", \"shriveling\", \"roots\"\n",
    "            ]\n",
    "\n",
    "# Read data from URL\n",
    "bob = pd.read_csv(url, names=col_names)\n",
    "color=list(bob)\n",
    "bob=bob.replace(\"?\",0)\n",
    "# bob=bob[color[1:]] \n",
    "# \n",
    "list_class=bob['class'].to_list()\n",
    "dict_class={}\n",
    "a=1\n",
    "for i in list_class:\n",
    " \n",
    "    if i not in dict_class.keys():\n",
    "        dict_class[i]=a\n",
    "        a+=1\n",
    "bob['class']=bob['class'].apply(lambda x: dict_class[x] )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictors, target =bob[color[1:]], bob['class'] #  делим на то что есть и на то что предсказать \n",
    "x_train , x_test , y_train, y_test = train_test_split(predictors, target, random_state=0) # 1А разбить выборку на тренировочную и тестовую \n",
    "X, y = train_test_split(bob, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 5, n_neighbors = 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Дмитрий\\Desktop\\МАГИСТР\\MagiCtr\\Интеллектуальные системы и технологии\\pr4\\pr4.ipynb Ячейка 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/%D0%94%D0%BC%D0%B8%D1%82%D1%80%D0%B8%D0%B9/Desktop/%D0%9C%D0%90%D0%93%D0%98%D0%A1%D0%A2%D0%A0/MagiCtr/%D0%98%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B%20%D0%B8%20%D1%82%D0%B5%D1%85%D0%BD%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D0%B8/pr4/pr4.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m SMOTE\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/%D0%94%D0%BC%D0%B8%D1%82%D1%80%D0%B8%D0%B9/Desktop/%D0%9C%D0%90%D0%93%D0%98%D0%A1%D0%A2%D0%A0/MagiCtr/%D0%98%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B%20%D0%B8%20%D1%82%D0%B5%D1%85%D0%BD%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D0%B8/pr4/pr4.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sm \u001b[39m=\u001b[39m SMOTE()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/%D0%94%D0%BC%D0%B8%D1%82%D1%80%D0%B8%D0%B9/Desktop/%D0%9C%D0%90%D0%93%D0%98%D0%A1%D0%A2%D0%A0/MagiCtr/%D0%98%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B%20%D0%B8%20%D1%82%D0%B5%D1%85%D0%BD%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D0%B8/pr4/pr4.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x_train, y_train \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39;49mfit_resample(x_train, y_train) \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_resample(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_ \u001b[39m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[1;32m--> 112\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_resample(X, y)\n\u001b[0;32m    114\u001b[0m y_ \u001b[39m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[39m1\u001b[39m], classes\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39munique(y)) \u001b[39mif\u001b[39;00m binarize_y \u001b[39melse\u001b[39;00m output[\u001b[39m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    118\u001b[0m X_, y_ \u001b[39m=\u001b[39m arrays_transformer\u001b[39m.\u001b[39mtransform(output[\u001b[39m0\u001b[39m], y_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:364\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    361\u001b[0m X_class \u001b[39m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[0;32m    363\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_k_\u001b[39m.\u001b[39mfit(X_class)\n\u001b[1;32m--> 364\u001b[0m nns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnn_k_\u001b[39m.\u001b[39;49mkneighbors(X_class, return_distance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[:, \u001b[39m1\u001b[39m:]\n\u001b[0;32m    365\u001b[0m X_new, y_new \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_samples(\n\u001b[0;32m    366\u001b[0m     X_class, y\u001b[39m.\u001b[39mdtype, class_sample, X_class, nns, n_samples, \u001b[39m1.0\u001b[39m\n\u001b[0;32m    367\u001b[0m )\n\u001b[0;32m    368\u001b[0m X_resampled\u001b[39m.\u001b[39mappend(X_new)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neighbors\\_base.py:749\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    747\u001b[0m n_samples_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_fit_\n\u001b[0;32m    748\u001b[0m \u001b[39mif\u001b[39;00m n_neighbors \u001b[39m>\u001b[39m n_samples_fit:\n\u001b[1;32m--> 749\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    750\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected n_neighbors <= n_samples, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    751\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but n_samples = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, n_neighbors = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_samples_fit, n_neighbors)\n\u001b[0;32m    752\u001b[0m     )\n\u001b[0;32m    754\u001b[0m n_jobs \u001b[39m=\u001b[39m effective_n_jobs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n\u001b[0;32m    755\u001b[0m chunked_results \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 5, n_neighbors = 6"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE()\n",
    "x_train, y_train = sm.fit_resample(x_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'kernel':['linear','rbf' ,  'poly', 'sigmoid'], 'C': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]}\n",
    "\n",
    "\n",
    "clf_1_SVC = svm.SVC(kernel='linear')\n",
    "clf_2_SVC = svm.SVC(kernel='rbf') \n",
    "clf_3_SVC = svm.SVC(kernel='poly')\n",
    "clf_predict=clf_1_SVC.fit(x_train,y_train)\n",
    "clf_predict_2=clf_2_SVC.fit(x_train,y_train)\n",
    "clf_predict_3=clf_3_SVC.fit(x_train,y_train)\n",
    "\n",
    "clf_SVC = svm.SVC()\n",
    "y_pred_SVM= clf_predict.predict(x_test)\n",
    "y_pred_SVM_2= clf_predict_2.predict(x_test)\n",
    "y_pred_SVM_3= clf_predict_3.predict(x_test)\n",
    "\n",
    "# grid_search= GridSearchCV(estimator=clf_SVC, param_grid=params)\n",
    "# grid_search.fit(x_train,y_train)\n",
    "# grid_search\n",
    "# print(str(grid_search.best_score_))\n",
    "# print(str(grid_search.best_estimator_))\n",
    "\n",
    "# y_pred_SVM= grid_search.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, algorithm='auto')\n",
    "knn_2 = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "knn_3 = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
    "\n",
    "knn_model = knn.fit(x_train, y_train)\n",
    "knn_model_2 = knn_2.fit(x_train, y_train)\n",
    "knn_model_3 = knn_3.fit(x_train, y_train)\n",
    "\n",
    "y_pred_KNN = knn.predict(x_test)\n",
    "y_pred_KNN_2 = knn_2.predict(x_test)\n",
    "y_pred_KNN_3 = knn_3.predict(x_test)\n",
    "\n",
    "\n",
    "params={'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "grid_search= GridSearchCV(estimator=knn, param_grid=params)\n",
    "grid_search.fit(x_train,y_train)\n",
    "print(str(grid_search.best_score_))\n",
    "print(str(grid_search.best_estimator_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100 , criterion='gini')\n",
    "random_forest_2 = RandomForestClassifier(n_estimators=100, criterion='entropy' )\n",
    "random_forest_3 = RandomForestClassifier(n_estimators=100 , criterion='log_loss')\n",
    "random_forest.fit(x_train, y_train)\n",
    "random_forest_2.fit(x_train, y_train)\n",
    "random_forest_3.fit(x_train, y_train)\n",
    "y_pred_RF = random_forest.predict(x_test)\n",
    "y_pred_RF_2 = random_forest_2.predict(x_test)\n",
    "y_pred_RF_3 = random_forest_3.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "params= {'criterion':['gini', 'entropy', 'log_loss'] }\n",
    "grid_search= GridSearchCV(estimator=random_forest, param_grid=params)\n",
    "grid_search.fit(x_train,y_train)\n",
    "print(str(grid_search.best_score_))\n",
    "print(str(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n##################Последнее задание ######################\\n')\n",
    "metrics = []\n",
    "models =  ['SVM' , 'KNN', 'Random_Forest']\n",
    "predictions=[y_pred_SVM, y_pred_KNN, y_pred_RF]\n",
    "\n",
    "for lab,i in zip(models, predictions):\n",
    "    precision, recall, fscore, _ = score(y_test, i, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, i)\n",
    "\n",
    "    \n",
    "    metrics.append(pd.Series({  'precision':precision, \n",
    "                                'recall':recall,\n",
    "                                'fscore':fscore,\n",
    "                                'accuracy':accuracy,}, name=lab))\n",
    "\n",
    "metrics = pd.concat(metrics, axis=1)\n",
    "\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_df1=pd.DataFrame(tsne_func(y_pred_SVM), columns=['x1','x2']) # y_pred_SVM_2\n",
    "x_df2=pd.DataFrame(tsne_func(y_pred_SVM_2), columns=['x1','x2']) \n",
    "x_df3=pd.DataFrame(tsne_func(y_pred_SVM_3), columns=['x1','x2']) \n",
    "tprint(\"TSNE   SVM\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('SVM linear')\n",
    "ax2.set_title('SVM rbf')\n",
    "ax3.set_title('SVM poly')\n",
    "\n",
    "sns.kdeplot(x_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(x_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(x_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(x_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(x_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(x_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_df1=pd.DataFrame(tsne_func(y_pred_KNN), columns=['x1','x2']) # y_pred_SVM_2\n",
    "y_df2=pd.DataFrame(tsne_func(y_pred_KNN_2), columns=['x1','x2']) \n",
    "y_df3=pd.DataFrame(tsne_func(y_pred_KNN_3), columns=['x1','x2']) \n",
    "tprint(\"TSNE   KNN\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('KNN auto') #'auto', 'ball_tree', 'kd_tree',\n",
    "ax2.set_title('KNN rbf')\n",
    "ax3.set_title('KNN ball_tree')\n",
    "\n",
    "sns.kdeplot(y_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(y_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(y_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(y_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(y_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(y_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "z_df1=pd.DataFrame(tsne_func(y_pred_RF), columns=['x1','x2']) # y_pred_SVM_2\n",
    "z_df2=pd.DataFrame(tsne_func(y_pred_RF_3), columns=['x1','x2']) \n",
    "z_df3=pd.DataFrame(tsne_func(y_pred_RF_3), columns=['x1','x2']) \n",
    "tprint(\"TSNE   RandomForest\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('RandomForest gini') # 'gini', 'entropy', 'log_loss'\n",
    "ax2.set_title('RandomForest entropy')\n",
    "ax3.set_title('RandomForest log_loss')\n",
    "\n",
    "sns.kdeplot(z_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(z_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(z_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(z_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(z_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(z_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_df1=pd.DataFrame(umap_function(y_pred_SVM), columns=['x1','x2']) # y_pred_SVM_2\n",
    "x_df2=pd.DataFrame(umap_function(y_pred_SVM_2), columns=['x1','x2']) \n",
    "x_df3=pd.DataFrame(umap_function(y_pred_SVM_3), columns=['x1','x2']) \n",
    "tprint(\"UMAP   SVM\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('SVM linear')\n",
    "ax2.set_title('SVM rbf')\n",
    "ax3.set_title('SVM poly')\n",
    "\n",
    "sns.kdeplot(x_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(x_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(x_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(x_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(x_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(x_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_df1=pd.DataFrame(umap_function(y_pred_KNN), columns=['x1','x2']) # y_pred_SVM_2\n",
    "y_df2=pd.DataFrame(umap_function(y_pred_KNN_2), columns=['x1','x2']) \n",
    "y_df3=pd.DataFrame(umap_function(y_pred_KNN_3), columns=['x1','x2']) \n",
    "tprint(\"UMAP   KNN\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('KNN auto') #'auto', 'ball_tree', 'kd_tree',\n",
    "ax2.set_title('KNN rbf')\n",
    "ax3.set_title('KNN ball_tree')\n",
    "\n",
    "sns.kdeplot(y_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(y_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(y_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(y_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(y_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(y_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "z_df1=pd.DataFrame(umap_function(y_pred_RF), columns=['x1','x2']) # y_pred_SVM_2\n",
    "z_df2=pd.DataFrame(umap_function(y_pred_RF_3), columns=['x1','x2']) \n",
    "z_df3=pd.DataFrame(umap_function(y_pred_RF_3), columns=['x1','x2']) \n",
    "tprint(\"UMAP   RandomForest\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize =(25, 5)) #linear rbf poly\n",
    "ax1.set_title('RandomForest gini') # 'gini', 'entropy', 'log_loss'\n",
    "ax2.set_title('RandomForest entropy')\n",
    "ax3.set_title('RandomForest log_loss')\n",
    "\n",
    "sns.kdeplot(z_df1['x1'], ax = ax1, color ='black')\n",
    "sns.kdeplot(z_df1['x2'], ax = ax1, color ='red')\n",
    "\n",
    "sns.kdeplot(z_df2['x1'], ax = ax2, color ='black')\n",
    "sns.kdeplot(z_df2['x2'], ax = ax2, color ='red')\n",
    "\n",
    "sns.kdeplot(z_df3['x1'], ax = ax3, color ='black')\n",
    "sns.kdeplot(z_df3['x2'], ax = ax3, color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
